
# MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation
**Authors:** Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao
**arXiv:2409.19483v4 [cs.CV] 16 Feb 2025**

## Abstract
Segmentation of anatomical structures and pathologies in medical images is essential for modern disease diagnosis, clinical research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. As a result, developing robust segmentation methods that require fewer labeled datasets remains a critical challenge. Recently, the introduction of foundation models like CLIP and SAM has paved the way for interactive and universal image segmentation. In this paper, we introduce **MedCLIP-SAMv2**, a novel framework that integrates CLIP and SAM to perform segmentation on clinical scans using text prompts, in both zero-shot and weakly supervised settings. Our approach includes fine-tuning the BiomedCLIP model with a new **Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE)** loss, and leveraging the **Multi-modal Information Bottleneck (M2IB)** to create visual prompts for generating segmentation masks with SAM. We also investigate using zero-shot segmentation labels in a weakly supervised paradigm. Extensive validation across breast tumor ultrasound, brain tumor MRI, lung X-ray, and lung CT demonstrates high accuracy.

## 1. Introduction
Deep learning (DL) techniques are SOTA in medical segmentation but face challenges: scarcity of annotated datasets, lack of interactivity/interpretability, and limited flexibility across modalities. Foundation models like CLIP and SAM offer solutions but require adaptation. Adapting CLIP to medical domains is non-trivial. We propose MedCLIP-SAMv2, upgrading our previous MedCLIP-SAM (MICCAI 2024) with:
1.  **M2IB** instead of gScore-CAM for saliency maps.
2.  **DHN-NCE loss** for fine-tuning BiomedCLIP.
3.  **Weakly Supervised Refinement** using nnU-Net with uncertainty estimation (checkpoint ensembling).
4.  **Advanced Prompt Engineering** using LLMs (GPT-4).
5.  Expanded validation including Lung CT.

## 2. Related Work
* **CLIP in Medical Domain:** Existing works (PubMedCLIP, MedCLIP) mostly focus on X-rays. BiomedCLIP is SOTA but its adaptability for zero-shot segmentation is underexplored.
* **Weakly Supervised Segmentation:** Techniques like CLIP-ES and SAMS exist for natural images. M2IB has shown promise for multi-modal saliency mapping.
* **SAM for Medical Imaging:** MedSAM and SAM-Med2D fine-tune SAM. We focus on using SAM in a zero-shot manner guided by BiomedCLIP.

## 3. Methods
The framework has three stages:
1.  **BiomedCLIP Fine-tuning (DHN-NCE):** Uses a new loss function that prioritizes hard negatives and decouples positive/negative terms to handle small batch sizes and subtle medical differences.
2.  **Zero-shot Segmentation:**
    * Extract image/text embeddings using fine-tuned BiomedCLIP.
    * **M2IB Module:** Generates saliency maps by maximizing mutual information between image and text while filtering irrelevant info.
    * **Prompt Generation:** Thresholds the saliency map (Otsu), applies connected component analysis, and extracts Bounding Boxes.
    * **SAM Inference:** Uses the BBox prompts to generate masks.
3.  **Uncertainty-Aware Weakly Supervised Segmentation:**
    * Uses zero-shot masks as pseudo-labels to train nnU-Net.
    * Uses **Checkpoint Ensembling** (saving weights from different cycles) to estimate uncertainty and refine predictions.

### 3.1 Efficient DHN-NCE Fine-tuning
Proposed loss function:
$$\mathcal{L}_{DHN-NCE} = \mathcal{L}^{v \rightarrow t} + \mathcal{L}^{t \rightarrow v}$$
It incorporates hardness weights ($W$) to penalize negative samples close to the anchor more heavily.

### 3.2 Zero-shot Segmentation Details
* **M2IB Equation:** $\Lambda_S = MI(Z_{img}, Z_{text}) - \gamma \times MI(Z_{img}, I)$.
* **Thresholding:** Otsu's method used to create binary mask $Y_{otsu}$.
* **Refinement:** Connected Component Analysis based on confidence scores.
* **SAM:** $Y_{zero-shot} = SAM(Y_{coarse}; V)$.

### 3.3 Uncertainty-Aware Weakly Supervised Segmentation
* Train nnU-Net on $(I, Y_{zero-shot})$.
* **Ensembling:** Average predictions from $G$ checkpoints to get $Y_{final}$ and compute entropy for uncertainty maps.

## 4. Results
* **Comparison:** MedCLIP-SAMv2 outperforms SOTA zero-shot methods (SaLIP, SAMAug) and few-shot methods (UniverSeg, ProtoSAM).
* **Saliency Maps:** M2IB significantly outperforms gScoreCAM and GradCAM.
* **Fine-tuning:** DHN-NCE loss yields better retrieval accuracy on ROCO dataset than InfoNCE, DCL, and HN-NCE.
* **Text Prompts:** Class-specific prompts work better for small tumors; generic prompts work better for large organs (lungs).
* **Visual Prompts:** Bounding boxes generally outperform points for SAM in this pipeline.

## 5. Discussion
MedCLIP-SAMv2 shows robust generalization across CT, MRI, Ultrasound, and X-ray. The combination of M2IB and DHN-NCE is crucial. Weakly supervised training with uncertainty estimation further refines segmentation, removing artifacts (e.g., in Lung CT).

## 6. Conclusion
MedCLIP-SAMv2 significantly improves segmentation performance with minimal supervision, demonstrating strong potential for clinical use in data-limited environments.